# -*- coding: utf-8 -*-
"""NlpLab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A_0eWuPVny9PsN-gwPk15XrTXLG_n0d4
"""

pip install nltk

import nltk

nltk.download('all')

import os
import nltk.corpus
print(os.listdir(nltk.data.find("corpora")))

from nltk.corpus import brown
print(brown.words)

word= brown.words()

for i in word[:500]:
  print(i,end=" ")

from nltk.corpus import gutenberg
print(gutenberg.fileids())

emma = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')
print(emma)
# for i in word[:100]:
#   print(i, end=" ")

ml="Machine learning (ML) is a type of artificial intelligence (AI) that allows software applications to become more accurate at predicting outcomes without being explicitly programmed to do so. Machine learning algorithms use historical data as input to predict new output values."

print(type(ml))

from nltk.tokenize import word_tokenize

ml_tokens= word_tokenize(ml)

print(ml_tokens)

from nltk.probability import FreqDist

fdist= FreqDist()

for word in ml_tokens:
  fdist[word.lower()]+=1
fdist

fdist['machine']

top10=fdist.most_common(10)
top10

from nltk.tokenize import blankline_tokenize
ml_bank=blankline_tokenize(ml)
len(ml_bank)

import numpy as np

def jaccardSimilarity(x, y):
    s1 = set(x)
    s2 = set(y)
    return float(len(s1.intersection(s2)) / len(s1.union(s2)))
    

x = [3,2,0,5]
y = [1,0,0,0]

xy = jaccardSimilarity(x,y)
print(xy)

p=ml_bank[0]
p

from nltk.tokenize import sent_tokenize
text="Hello, Mr. welcome to my univesity. how are you ??"
print(sent_tokenize(text))

from nltk.util import bigrams,trigrams,ngrams
import nltk
mltoken= nltk.word_tokenize(ml)
mltoken
mlbigram=list(nltk.bigrams(mltoken))

mlbigram

mlt="Machine learning (ML) is a type of artificial intelligence (AI) that allows software applications to become more accurate at predicting outcomes without being explicitly programmed to do so. Machine learning algorithms use historical data as input to predict new output values.mtb="

mlb=list(nltk.bigrams(mlt))

mlb

mltr=list(nltk.trigrams(mlt))

mltr

mlb=list(nltk.trigrams(ml))

mlb

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
stop_words=set(stopwords.words("english"))
print(stop_words)

text="Hello Mr.Smith, how  are you? This is GLA University. We are studying natural language processing."
words=word_tokenize(text)
filtered_sentence=[];
for w in words:
  if w not in stop_words:
    filtered_sentence.append(w)
print(filtered_sentence)

"""**Stemming**"""

from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
ps=PorterStemmer()
example_words=['python','pythoner','pythoned','pythonly']
for w in example_words:
  print(ps.stem(w))

from nltk.stem.snowball import SnowballStemmer
sst=SnowballStemmer("english")
wordstostem=["give","given","giving","gave"]
for w in wordstostem:
  print(w+" : "+sst.stem(w))

from nltk.stem import LancasterStemmer
from nltk.tokenize import word_tokenize
ps=LancasterStemmer()
example_words=['python','pythoner','pythoned','pythonly']
for w in example_words:
  print(ps.stem(w))

import numpy as np
import math
def cosine_similarity(v1,v2):
    sumxx, sumxy, sumyy = 0, 0, 0
    for i in range(len(v1)):
        x = v1[i]
        y = v2[i]
        sumxx += x*x
        sumyy += y*y
        sumxy += x*y
    return sumxy/math.sqrt(sumxx*sumyy)

x = [3,2,0,5]
y = [1,0,0,0]

xy = cosine_similarity(x,y)
print(xy)

from nltk.corpus import state_union
print(state_union.raw('2006-GWBush.txt'))

